# 目录

| 序号 | 实践名称                                            | 难度  | 备注     |
| ---- | --------------------------------------------------- | ----- | -------- |
| 1✅   | 实践2-1 基于生成对抗网络的sin曲线样本模拟           | 🌟     | 应用安全 |
| 2    | 实践2-2 基于对抗性攻击无数据替代训练的模型窃取      | 🌟🌟🌟🌟🌟 | 内生安全 |
| 3    | 实践3-1 基于卷积神经网络的数据投毒攻击              | 🌟🌟🌟   | 应用安全 |
| 4✅   | 实践3-2 基于卷积神经网络的人脸活体检测              | 🌟🌟    | 赋能安全 |
| 5✅   | 实践3-3 基于卷积神经网络的验证码识别                | 🌟🌟    | 赋能安全 |
| 6✅   | 实践4-1 基于对抗样本生成算法的图像对抗              | 🌟🌟    | 应用安全 |
| 7✅   | 实践5-1 基于随机森林算法的图像去噪                  | 🌟🌟    | 赋能安全 |
| 8✅   | 实践6-1 基于贝叶斯和SVM分类算法的垃圾邮件过滤       | 🌟     | 赋能安全 |
| 9✅   | 实践7-1 基于双向LSTM模型的网络攻击检测              | 🌟🌟    | 赋能安全 |
| 10✅  | 实践8-1 基于梯度下降算法的模型逆向攻击              | 🌟🌟🌟   | 内生安全 |
| 11   | 实践9-1 基于深度伪造技术的人脸识别                  | 🌟🌟🌟   | 应用安全 |
| 12   | 实践10-1 基于影子模型的成员推理攻击                 | 🌟🌟    | 内生安全 |
| 13   | 实践11-1 基于神经网络的属性推理攻击                 | 🌟🌟🌟   | 内生安全 |
| 14   | 实践12-1 模型公平性检测和提升                       | 🌟🌟🌟   | 内生安全 |
| 15   | 实践13-1 基于Skip Encoder-Decoder网络的图像水印去除 | 🌟🌟    | 赋能安全 |
| 16   | 实践14-1 基于Tacotron2的语音合成                    | 🌟🌟🌟   | 应用安全 |
| 17   | 实践15-1 基于YOLOv5的安全帽识别                     | 🌟🌟🌟🌟  | 赋能安全 |
| 18   | 实践16-1 基于图神经网络的代码漏洞检测               | 🌟🌟🌟🌟🌟 | 赋能安全 |



# 实践2-1	基于生成对抗网络的sin曲线样本模拟

## 遇到的问题

1. 导入包即使pip install了也找不到

   解决：换成vscode即可

2. 完全按照书本上的代码编写，发现完全不收敛，无法模拟sin曲线样本

   ![image-20250927150802153](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20250927150802153.png)

   解决：严格按照教材中的Python和库版本进行实验。最后得到的结果为：7999 0.5 0.5099999904632568
   
   <img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20250927183242992.png" alt="image-20250927183242992" style="zoom:50%;" />
   
   

## 总结

生成对抗网络需要构建三个模型：

1. 生成器：根据网络中的神经元生成数据，目的是要生成“看起来像”的数据，如图像、音频、文本等。本质是一个神经网络，将随机分布映射到真实分布
2. 判别器：判断数据是否是真实的，还是生成器造出来的。本质是一个二分类器
3. 生成对抗网络模型：专门用来训练生成器。主要是将二者组合，同时冻结判别器，激活生成器的参数，通过判别器的结果让生成器调参进化。

# 实践3-2 基于卷积神经网络的人脸活体检测

## 实验结果

1. 训练结果：![image-20251002200059377](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251002200059377.png)

2. 可以识别非真人和真人

   ![image-20251002212024586](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251002212024586.png)

## 遇到的问题

`(.venv_tf_2.4.1) (base) PS E:\self-cultivation\Python\人工智能安全> pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/ Writing to C:\Users\29263\AppData\Roaming\pip\pip.ini` 的作用是什么？

解决：其实就是一个加速的作用。设置镜像源后，后续使用`pip install`命令安装包时，会默认从阿里云镜像源下载，而不是直接从官方PyPI网站下载，从而加快下载速度（尤其是在国内网络环境下）。

![image-20251002142254323](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251002142254323.png)

解决：发现其实是因为训练集中没有数据，需要提前划分训练集和测试集。但是这里其实是用train.txt来保存所有文件路径的，格式是 `图片路径,标签`，询问ai，明确需要打开文件进行处理：

~~~python
with open(r"实践3-2\NUAA\train.txt", "r", encoding="utf-8") as f:
    lines = f.readlines()
    random.seed(42)
    random.shuffle(lines)  # 随机打乱

    for line in lines:
        path, label = line.strip().split(',')
        # 路径兼容
        path = path.replace('/', os.sep)
        image = cv2.imread(path)
        if image is None:
            print(f"无法读取图片: {path}")
            continue
        image = cv2.resize(image, (img_height, img_width))
        image = img_to_array(image)
        data.append(image)
        labels.append(int(label))
~~~

这样就可以成功运行，但是出现了中文路径无法识别的问题，修改即可。

`ValueError: Could not interpret optimizer identifier: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x000002A978C34CD0>`

解决：1️⃣在运行 `utils.py` 时，`from train import model` 会**执行 train.py 的全部代码**，包括模型的训练、编译等。这样会导致 **Keras/TensorFlow** 在模型对象跨文件引用时，优化器对象无法被正确识别和反序列化，从而报出 `Could not interpret optimizer identifier` 这样的错误。
所以需要先单独运行 trian.py，并添加保存模型的代码：`model.save('Lab3-2/model.h5')`
然后再在 utils.py 中调用这个 .h5 ，而不是 `from train import model` 
2️⃣兼容性问题：Keras 2.x 和 TensorFlow 2.x 的 API 有兼容性问题。因为 tf 和 keras 混用导致的，统一风格，将 `keras` 开头的 改为 `tensorflow.keras`。如：`from keras_preprocessing.image import img_to_array,ImageDataGenerator`➡️`from tensorflow.keras.preprocessing.image import img_to_array, ImageDataGenerator`。`train.py` 和 `model.py` 都要改。

只运行了一个epoch。![image-20251002160049440](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251002160049440.png)

解决：1️⃣因为这里的 `steps_per_epoch`设置不合理，应该是 `len(trainX)//BS`，`steps_per_epoch`的含义是 每个 epoch（轮）要训练多少个 batch。如果设置过大，之后的轮就无数据可用了。一般都设置为 `len(trainX)//BS` 

~~~python
H=model.fit(aug.flow(trainX,trainY,batch_size=BS),
    validation_data=(testX,testY),steps_per_epoch=len(trainX),# ➡️len(trainX)//BS
    epochs=EPOCHS,verbose=1)
~~~

2️⃣其实是因为 **TensorFlow GPU 版本和你系统上的 CUDA/cuDNN 不匹配**，导致训练过程在调用 GPU 底层库时直接崩溃，没有机会抛出 Python 异常，所以程序在 Epoch 1 就退出了。这里直接禁用GPU即可：

~~~python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # 禁用所有 GPU
~~~

蓝框不居中：<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251002201210194.png" alt="image-20251002201210194" style="zoom: 50%;" />

解决：调整了一下蓝框范围：![image-20251002204540477](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251002204540477.png)

## 总结

本次实验确实难度更大一些，主要是兼容性有点难搞，没有书中的版本，只好自己摸索，好在问题都顺利解决了。

本次实验分为三个文件：

~~~python
model.py	# 用于定义模型架构
train.py	# 用于训练模型
utils.py	# 更像是main文件，用于打开摄像头，调用训练好的模型进行实质性应用等
~~~

**model.py**：这部分没什么好说的，就是根据VGG架构构建一个模型
**train.py**：这部分主要包括，数据预处理、模型训练，将数据打包成模型可以接受的样子进行训练并保存
**utils.py**：这部分就有点复杂了，调用了很多生疏的函数，主要是对视频和显示做了很多工作，然后再调用模型 `(real,fake)=model.predict(image)[0]`，其实也就这样，然后把模型给出的结果进行一个小加工输出即可

# 实践3-3 基于卷积神经网络的验证码识别

## 实验结果

![image-20251008175655200](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251008175655200.png)



## 遇到的问题

1. ![image-20251008154755174](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251008154755174.png)

   解决：需要自定义一个 common.py 文件，定义相关全局变量或者函数等

2. ![image-20251008170219017](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251008170219017.png)

   解决：由于 PyTorch 2.6 版本中对`torch.load()`函数的默认行为做了更改导致的。

   1. PyTorch 2.6 将`torch.load()`的`weights_only`参数默认值从`False`改为了`True`
   2. 当`weights_only=True`时，PyTorch 会限制只能加载模型权重，不允许加载自定义类或函数（出于安全考虑，防止恶意代码执行）
   3. 你的模型使用了自定义的`model.mymodel`类，而这个类不在 PyTorch 默认的安全加载列表中，因此加载失败

   解决方法：修改加载模型的代码，显式设置`weights_only=False`，即 `m = torch.load("model.pth", map_location="cpu", weights_only=False).to(device)`

## 总结

本试验是基于CNN模型进行的验证码识别实验。首先是模型构建，然后生成验证码图片，第三步则是对这些验证码图片进行预处理（datasets），最后再进行训练并保存模型。

其实一开始正确率不高，基本上全错，之后才发现是因为训练集数量太少了，我只用了700个作为训练集。之后用1w个之后就明显好了很多，达到72%。如果进一步增加训练集正确率一定会增加不少。

此外本项目的一个难点是不太清楚书上的代码具体文件是什么。比如那个 import common 我一开始以为是一个包，但是其实是自己写的，那个 one-hot 函数也是。但是最后虽然命名不同，还是顺利解决。

# 实践4-1 基于对抗样本生成算法的图像对抗

## 思考

**对抗样本生成算法 🆚 对抗生成网络**

| 对比维度     | 对抗样本生成算法                              | 对抗生成网络（GAN）                           |
| ------------ | --------------------------------------------- | --------------------------------------------- |
| **核心目标** | 生成带微小**扰动**的 “对抗样本”，欺骗已有模型 | 生成逼真的 “**全新假数据**”，模仿真实数据分布 |
| **技术核心** | 利用目标模型的梯度 / 决策漏洞，计算最小扰动   | 生成器（G）与判别器（D）的对抗训练与博弈      |
| **依赖条件** | 必须依赖已训练好的 “目标模型”（如分类器）     | 仅依赖真实数据，无需预设目标模型              |
| **输出产物** | 基于原始数据修改的 “扰动样本”（非全新数据）   | 脱离原始数据、独立生成的 “全新假数据”         |
| **训练需求** | 无需训练新模型，多为 “一次性计算”             | 需长期训练双网络（G 和 D），优化成本高        |
| **安全性**   | 多用于 “攻击” 模型，暴露模型鲁棒性问题        | 多用于 “生成” 数据，无直接攻击属性            |
| **典型应用** | 模型鲁棒性测试、隐私保护（如对抗脱敏）        | 数据增强、图像生成（如人脸）、风格迁移        |
| **核心关联** | 两者均涉及 “对抗” 逻辑，但对抗对象完全不同    | 两者均涉及 “对抗” 逻辑，但对抗对象完全不同    |

**总结**：FGSM 和 PGD 本质上都是通过对输入数据（比如图片、文本）做 “微小手脚”，来达到让模型失效（做出错误判断）的目的

做手脚的前提：“人眼看不出来”
让模型失效的目的：大多数时候不是为了 “搞破坏”，而是为了 “测试模型”。像 “模型的压力测试工具”—— 通过故意让模型 “出错”，帮我们找到它的弱点，再针对性改进，让最终的模型更稳定、更抗干扰。

## 实验结果

FGSM

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251008203742912.png" alt="image-20251008203742912" style="zoom:60%;" />

FGSM vs PGD

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251008211800086.png" alt="image-20251008211800086" style="zoom:60%;" />

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251008210642417.png" alt="image-20251008210642417" style="zoom:50%;" />



## 遇到的问题

1. 不存在 `pretrained_model`：这个模型应该是已经训练好的分类器，但是我在资料包中没有看到相关的文件

   解决：在网上找相关训练好的模型

## 总结

本次实验实现还是比较简单的，但是理解可能有点难。下面我做一个简要的总结。

**FGSM：**通过 梯度符号（`data_grad.sign()`）来确定扰动方向（.sign用来保留符号），若某个像素的梯度为**正值**：调亮该像素会使模型损失增大（更可能认错）反之则降低亮度（就是让他绝对值变大）。特点是不管扰动后像素之间的相互影响，按初始梯度统一调整

**PDG：**改一次（和FGSM一样改法）看一眼（让模型判断，得到梯度信息），每次都往当前最能让模型犯错的方向改（最终用最小的总扰动让模型彻底认错）。



#   实践5-1 基于随机森林算法的图像去噪

## 实验结果

普通版：

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011162009842.png" alt="image-20251011162009842" style="zoom:50%;" />

分析：当树增多之后，相应的时间会急剧增加，而且 RMSE 降低幅度减缓，甚至出现不如前者的情况，综合来看取10或者30效果最好

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011161941882.png" alt="image-20251011161941882" style="zoom:50%;" />

![image-20251011162256279](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011162256279.png)

加强版：

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011164240647.png" alt="image-20251011164240647" style="zoom:60%;" />

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011164328706.png" alt="image-20251011164328706" style="zoom:50%;" />

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011164359572.png" alt="image-20251011164359572" style="zoom:60%;" />

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011164429496.png" alt="image-20251011164429496" style="zoom:67%;" />

## 遇到的问题

1. ![image-20251009111831066](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251009111831066.png)

   解决：表示只接受一个参数，但是这里传了两个。直接用 `[ ]` 装裱起来即可
   
2. ![image-20251009120037101](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251009120037101.png)

   解决：问题出在**图像的通道类型不符合 `THRESH_OTSU` 阈值化的要求**。错误提示 `THRESH_OTSU mode: src_type == CV_8UC1 || src_type == CV_16UC1` 说明：`cv2.THRESH_OTSU` 模式要求输入图像必须是**单通道灰度图**（8 位或 16 位），但代码中传入的是**3 通道彩色图**（CV_8UC3）。因为代码中存在一个变量名拼写错误（Image写成Iamge），导致灰度图转换未生效。

3. ![image-20251009120401475](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251009120401475.png)

   解决：是因为csv本身没有 `write` 这个属性，需要先打开文件，初始化csv对象，再使用这个方法
   
4. ![image-20251009205103764](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251009205103764.png)

   解决：问题是特征对不上号。我进行了如下测试：
   首先在 `build_features.py` 中，生成 .csv 后添加

   ~~~python
   # 加载CSV数据
   data = np.loadtxt("./Lab5-1/features.csv", delimiter=",")
   
   # 拆分特征X和目标值y
   y = data[:, 0]
   X = data[:, 1:]
   
   # --------------------------
   # 新增：打印特征维度（关键检查）
   # --------------------------
   print("特征矩阵X的形状（样本数, 特征数）：", X.shape)  # 核心打印，看第二个数字是24还是25
   print("单个样本的特征数：", X.shape[1])  # 直接打印特征维度（列数）
   print("CSV数据总行数（样本数）：", data.shape[0])  # 可选，确认样本数量
   print("CSV数据总列数（1个目标+N个特征）：", data.shape[1])  # 可选，看总列数是否为25（1+24）或26（1+25）
   ~~~

   确认当前生成的情况，得到：

   ~~~python
   特征矩阵X的形状（样本数, 特征数）： (568181, 24)
   单个样本的特征数： 24
   CSV数据总行数（样本数）： 568181
   CSV数据总列数（1个目标+N个特征）： 25
   ~~~

   从中可以明确：**当前生成的`features.csv`是完全正确的**（1 个目标值 + 24 个特征，总列数 25），这与模型训练时期望的 24 维特征完全匹配。问题应该出在`denoise_document.py`中。
   随后我又验证了输入的图像格式是否一致，得到：

   ~~~python
   trainImage.shape: (424, 544)
   testImage.shape: (424, 544)
   ~~~

   说明是一致的，但是发现：

   ~~~python
   roiFeatures.shape: (226800, 25)
   ~~~

   和特征矩阵X的形状有区别，多了1，说明问题出现在中间这个过程：

   ~~~python
   # denoise_document.py
   # 提取图像特征
   roiFeatures=[]
   for y in range(0,image.shape[0]):
       for x in range(0,image.shape[1]):
           roi=image[y:y+5,x:x+5]
           (rH,rW)=roi.shape[:2]
           if rW!=5 or rH!=5:
               continue
   
           features=roi.flatten()
           roiFeatures.append(features)
           
   # build_features.py
   # 遍历每个像素
   for y in range(0,trainImage.shape[0]):
       for x in range(0,trainImage.shape[1]):
           trainROI=trainImage[y:y+5,x:x+5]    # 每个像素的 5x5 区域被视为一个特征样本
           cleanROI=cleanImage[y:y+5,x:x+5]
           (rH,rW)=trainROI.shape[:2]
           if rW!=5 or rH!=5:
               continue
   
           features=trainROI.flatten()
           target=cleanROI[2,2]
   
           if random.random()<config.SAMPLE_PROB:
               # 构造行：[目标值] + [特征列表]（纯数值列表）
               row = [target] + features
               # 使用csv.writer写入
               writer.writerow(row)
   ~~~

   最终找到 问题根源：`build_features.py` 中的随机采样导致 CSV 列数 “被误解”。**当 `features` 是 numpy 数组时，`[target] + features` 会触发 “维度拼接异常”，导致写入 CSV 的特征列数少了 1 列（从 25 维变成 24 维）**（其实这个也挺难办的，之前在这里遇到报错，所以没有按照书上的写，而是改成当前这样）

   还需要修改一点是模型的期望输入，这里生成正确的 features.csv 后，train 这个文件训练模型时用了错误的 csv，同样需要重新训练，才能更新模型的输入。

## 总结

个人感觉本次实验还是有难度的，具体思路是，训练模型+应用模型。训练模型需要对图像进行预处理（如灰度化，裁剪，resize等），然后进行特征提取，生成csv文件，然后设置 `RandomForestRegressor`初始化 `n_estimators`等进行训练，这里记录了训练时间以及均方差对应树的数量关系。训练完成后保存模型，方便应用模型。应用模型过程也比较简单，首先是需要对图片进行预处理，特征提取，让其特征和训练时特征一致，然后将图像输入模型中进行降噪，最后对降噪后的模型进一步处理得到图像格式输出保存即可。然后优化部分主要是可视化，这里主要参考AI的一个应用。

# 实践6-1 贝叶斯和SVM分类算法的安全应用

## 实验结果

![image-20250929151334675](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20250929151334675.png)

![image-20250929154651804](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20250929154651804.png)

![image-20250929153922240](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20250929153922240.png)

![image-20250929153929421](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20250929153929421.png)

## 遇到的问题

1. pytorch 1.7.0 找不到

   解决：不是直接使用 `pip install pytorch==1.7.0`，而是 `pip install torch==1.7.0`。而且好像找不到1.7.0，最低 1.7.1，这里直接安装 1.7.1 版本了
   
2. ![image-20250929150851993](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20250929150851993.png)

   解决：经过查询，发现其实是因为Numpy版本过高，卸载后根据教材中的版本安装即可

   ![image-20250929151334675](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20250929151334675.png)

## 总结

本次实验使用两个机器学习模型对同一个数据集进行训练和预测，用于垃圾邮件分类。首先通过设计函数将email文件的数据进行提取，主要是对其中的单词进行统计（词频）、清理（删除单个单词和标点数字等），然后对这些单词使用另一个函数进行特征提取，构造能够放入两个模型的特征矩阵。然后调包，将这些数据输入，即可成功训练模型。模型训练结束后即可放入测试集进行验证。

输出的混淆矩阵的含义如下：（以第一个混淆矩阵为例）

```
实际正类\预测正类: 126 (TP)  |  实际正类\预测负类: 4 (FN)
实际负类\预测正类: 6 (FP)    |  实际负类\预测负类: 124 (TN)
```

从这个矩阵可以计算出：

- 准确率(Accuracy) = (TP+TN)/(TP+FP+FN+TN) = (126+124)/(126+4+6+124) = 250/260 ≈ 96.15%
- 精确率(Precision) = TP/(TP+FP) = 126/(126+6) ≈ 95.45%
- 召回率(Recall) = TP/(TP+FN) = 126/(126+4) ≈ 96.92%

# 实践7-1 基于双向LSTM模型的网络攻击检测

## 实验结果

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011210057249.png" alt="image-20251011210057249" style="zoom:50%;" />

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011210106295.png" alt="image-20251011210106295" style="zoom:50%;" />

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011210816774.png" alt="image-20251011210816774" style="zoom:50%;" />

~~~python
模型准确率: 0.9669
模型召回率: 0.9945
~~~



## 遇到的问题

1. ![image-20251011203551093](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011203551093.png)

   解决：显示无法转换 string --> float。我具体查看了一下csv文件，是这样的格式：

   ~~~Python
   [[206 20 192 ... 16697 0.0 'normal']
    [60 20 40 ... 7504 0.000537 'normal']
    [60 20 40 ... 7504 0.000155 'normal']
    ...
    [154 20 140 ... 64030 6e-06 'attack']
    [60 20 40 ... 9140 0.001324 'attack']
    [62 20 48 ... 0 0.0 'attack']]
   ~~~

   可以看到最后一列是标签的形式，发现是之前特征定义出现了问题，经过检查，应该删除最后的 `'class'`，就可以了。

2. ![image-20251011205806052](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251011205806052.png)

   解决：问题在于GPU不可用，这里在开头导入：

   ~~~python
   import os
   os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
   ~~~

   强制关闭GPU即可。

## 总结

感觉这次的实验还是比较简单的，流程同样是构建模型，数据预处理，特征提取，训练模型，评估模型。值得一提的是LSTM模型和普通CNN的区别，在于其有一个 `Bidirectional(LSTM(64, ...)),  # 双向LSTM层（核心）`表示其是一个双向的LSTM层，后续的层倒是比较常规。下面对比一下 **LSTM 和 CNN 模型构造上的区别：**

~~~python
# 双向LSTM
Sequential([
    Bidirectional(LSTM(64, ...)),  # 双向LSTM层（核心）
    Dense(128, ...),               # 全连接层
    Dense(1, activation="sigmoid") # 输出层（二分类）
])
# 擅长序列数据（1D 时序数据），如：网络流量时序、文本、传感器数据等。
# 捕捉时间 / 顺序依赖关系（如 “先出现 A，后出现 B” 的规律）。


# CNN
Sequential([
    Conv2D(32, (3,3), activation='relu'),  # 卷积层（核心）
    MaxPooling2D((2,2)),                   # 池化层
    Flatten(),                             # 展平层
    Dense(128, activation='relu'),         # 全连接层
    Dense(1, activation='sigmoid')         # 输出层
])
# 擅长网格 / 空间数据（2D/3D），如：图片（2D 像素网格）、视频帧、医学影像等。
# 捕捉空间局部相关性（如 “相邻像素构成边缘” 的规律）。
~~~











#   实践10-1 基于影子模型的成员推理攻击

## 快问快答

下面是对成员推理攻击的一些疑问和AI回答：

~~~python
# 1.请简要介绍一下成员推理攻击原理
核心思想：机器学习模型在训练过程中会 “记住” 训练数据的某些特征，使得模型对训练集样本和非训练集样本的响应存在细微差异，如预测置信度、中间层输出等。攻击者通过分析这些差异，来推断数据是否属于训练集。

# 2.但是就算判断出是来自训练集or非训练集之后呢，哪里体现出 “攻击” 呢？对什么有损害呢？
主要体现在对隐私和模型本身两个方面：
1. 精准锁定并泄露个体隐私
    个人敏感信息暴露：例如，在医疗 AI 系统中，若攻击者推断出某一医疗记录是训练集成员，就可能知晓该患者患有某种特定疾病，从而泄露患者的健康隐私。
    数据重建风险：比如在图像识别模型中，攻击者可能通过推断出的训练集图像信息，逐步还原出人脸图像等敏感内容，进一步侵犯个人隐私
2. 为后续攻击提供 “精准弹药”
	确定数据归属后，攻击者不再盲目攻击，而是能针对训练集的特征设计更有效的攻击，破坏模型安全。比如投毒攻击，模型规避等。
3. 削弱数据与模型的核心价值
	知识产权受损：攻击者可通过大量 “训练集成员” 数据，反向推导训练集的统计特征（如数据分布、核心维度），甚至复制出功能相似的模型，侵犯原开发者的权益。
	数据可信度下降：若用户知晓自己的数据被判定为训练集成员，且存在泄露风险，可能会拒绝提供数据，导致模型后续的训练数据质量下降，影响模型迭代。
    
# 3.但是非训练集成员的数据难道就不具有敏感信息了吗？比如说，进行预测的时候，测试集的信息不也有个人敏感信息吗
引用教材的解释：成员推理攻击可以使攻击者获得训练数据集所共有的特征，比如，对于一个由医院患者的体态数据集训练的模型，攻击者在只拥有数据的情况下无法判断一个人是否生病，但是如果可以通过成员推理攻击得出数据来自此模型，则可以知道一个人是否生病。

# 4.成员推理攻击方法--影子模型攻击的原理和思路
1.你确实不知道目标模型的训练数据，但你能找到一批 “和目标训练数据长得很像” 的数据（比如目标用医院 A 的糖尿病数据，你找医院 B 的糖尿病数据）。
2.你用这批 “相似数据” 训练多个影子模型（和目标模型结构、训练方法一致），此时每个影子模型都有了 “自己的训练数据” 和 “没见过的陌生数据”。
3.影子模型对 “自己的训练数据” 预测置信度高，对 “陌生数据” 置信度低 —— 你把这些 “高置信度数据标为 in（影子的训练数据）、低置信度标为 out（影子的非训练数据）”，用这些带标签的数据去训练攻击模型，让它学会 “看置信度辨数据归属”。
4.最后用攻击模型去分析 “目标模型对某数据的置信度”，才能判断该数据是不是 “目标模型的训练数据”，而不是直接用影子模型的置信度下结论。

# 5.如果不知道目标模型的结构、参数、训练过程等，又该如何构建影子模型呢？
影子模型的核心优势恰恰是 “不需要完全知道目标模型的细节”，攻击者可以通过 “黑盒观察” 和 “合理假设” 来构建出足够相似的影子模型，核心在于 “模仿行为” 而非 “复制内部结构”。
1.基于公开信息的 “合理假设”
比如目标模型是文本分类类（如垃圾邮件识别），常用的是 RNN 或 Transformer，攻击者就选择这类结构。
2.通过 “黑盒查询” 反向试探
比如提交不同类型的图片，看目标模型的分类结果和置信度，判断它可能是 “轻量级模型”（推理快、置信度差异小）还是 “复杂模型”（推理慢、置信度差异大），再调整影子模型的复杂度。
3.利用 “数据分布相似性” 弥补模型差异
比如目标模型用 “某地区用户的消费数据” 训练，攻击者找不到该地区数据，但可以用 “同收入水平、同年龄段的其他地区用户数据” 训练影子模型。

# 6. 为什么要对不同层进行初始化 def init_params(m) ? 我之前搭建模型的时候好像没有用上这些，也没有区分这么细致
默认初始化为什么“也能跑”：
model = nn.Sequential(
    nn.Conv2d(3, 16, 3),
    nn.ReLU(),
    nn.Linear(128, 10)
)
这种，PyTorch 内部自动帮你做了默认初始化。
什么时候需要自己写初始化函数？
✅ 需要时机
你实现的是新网络结构（别人没定义过）；
你组合了不同激活函数层、卷积层、BN层；
你想复现论文中的初始化方式（论文经常写 “all conv layers initialized with He normal”）；
你想控制训练稳定性，比如做攻击实验、对比实验（像你现在研究的成员推理攻击）。
❌ 不需要时机
你只是用 torchvision 的标准模型（ResNet、VGG 等），它们已经自带初始化；
你用的小模型结构浅、训练能稳定收敛。
这里进行初始化，可能是老师想让我们多了解一些模型定义的方法。

# 7.我记得我之前训练的时候直接有包装好的函数带入就行，为什么这里还需要专门定义 train_per_epoch和val_per_epoch这种？
在很多高层框架（比如 Keras、PyTorch Lightning、fastai 等）里，确实可以一句话就训练模型，比如：
model.fit(train_loader, epochs=10, validation_data=val_loader)
但是 PyTorch 是一种 高度灵活的框架，不像 Keras 那样帮你包好所有逻辑，所以得写训练啊验证逻辑这样
~~~

## 实验结果

1.python ./Lab10-1/cli.py membership-inference pretrained-dummy
<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251013214632919.png" alt="image-20251013214632919" style="zoom: 67%;" />

2.python ./Lab10-1/cli.py membership-inference pretrained-dummy --dataset MNIST

<img src="C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251014180002198.png" alt="image-20251014180002198" style="zoom:67%;" />



## 遇到的问题

1. ![image-20251013185430217](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251013185430217.png)

   解决：问题出在将 membership_inference() 定义为普通的函数，应该在其上面加上一个 `@click.group()` 装饰器。

   ~~~Python
   @cli.group(help='Membership Inference Attack commands')
   def membership_inference():
       """Membership Inference Attack commands"""
       pass
   ~~~

2. ![image-20251013204122097](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251013204122097.png)

   解决：问题出在 `posteriors=F.softmax(outputs,dim)` 这里，接受到的 outputs 为 ‘NoneType’ 。说明 outputs 为空，造成的原因最可能是 `model(x)` 没有返回值。经过检查发现，forward 函数中有一个没有 return out，添加上即可。

3. ![image-20251013210144293](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251013210144293.png)

   解决：参数对应错位。检查：

   ~~~Python
   # 定义处
   def train_per_epoch(model,iterator,optimizer,device,bce_loss=False):
   
   # 调用处
   train_loss,trian_acc=train_per_epoch(model,train_loader,criterion,optimizer,device)
   train_loss,train_acc=train_per_epoch(model,train_loader,loss,optimizer,device)
   
   # 教材定义
   def train_per_epoch(model,iterator,criterion,optimizer,device,bce_loss=False):
   ~~~

   发现是少了一个 `criterion` 补齐即可。

4. ![image-20251013211723265](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251013211723265.png)

   解决：报错的意思是，我的攻击模型输出了 **两个类别（形状 [batch_size, 2]）**，而我的标签（target）是 **一维的 [batch_size]**，但我使用的损失函数是 **`nn.BCEWithLogitsLoss()`（二进制交叉熵）**，这个函数要求输入输出形状完全相同（例如 [batch_size] 对 [batch_size]，或 [batch_size, 1] 对 [batch_size, 1]）。经过检查，发现是我之前因为错误3的问题而额外增加了一个定义：`    criterion = nn.BCEWithLogitsLoss()  # BCE 损失函数实例`，现在将其删除即可。

5. ![image-20251013213754705](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251013213754705.png)

   解决：入截图所示，这里出现的问题是 将 append 写成了 extend，改过来即可。
   
6. ![image-20251014172507862](C:\Users\29263\AppData\Roaming\Typora\typora-user-images\image-20251014172507862.png)
   解决：意思是 Torchvision 想从官方源（`https://ossci-datasets.s3.amazonaws.com/mnist/`）下载 MNIST 数据集，但下载被中断（SSL错误)，备用源也返回了 502。多重复几次就行，可能是网卡了，记得翻墙，如果翻墙还出现这个问题，可以试试切换代理，换速度快的。

## 总结

本次实验感觉还是有一定难度，主要是代码量挺大的，而且不太熟悉命令行参数的使用。这里先总结一下流程：

1. 下载数据集（CIFAR10 或 MNIST），把训练集随机划分成 4 份：shadow_train / shadow_out / target_train / target_out。
2. 构建并 **训练或加载** TargetNet（目标模型）和 ShadowNet（影子模型）。
3. 用 `prepare_attack_data` 从 Target/Shadow 的 train/out（成员/非成员）上收集模型输出后验 `posteriors`（或 top-k），并打上 membership 标签（1=member, 0=non-member）。
4. 用 shadow 的 (posteriors, label) 训练攻击器 AttackMLP（`train_attack_model`）。
5. 用训练好的攻击器在 target 的 (posteriors, label) 上做推断，评估攻击成功率与 precision/recall/f1（`attack_inference`）。

其实我个人认为最主要的是摸清不同模型的输入输出（以**CIFAR10**为例）：

~~~Python
# TargetNet / ShadowNet
# 输入
x：输入图像的 mini-batch
	shape：[batch_size, channels, height, width]
	CIFAR10: channels=3, height=32, width=32 （RGB三通道，高为32，宽为32）（举个例子，不一定是真的）
举例：batch_size = 4 → x.shape = [4, 3, 32, 32]

# 输出
outputs：
    shape：[batch_size, num_classes]
    CIFAR10: num_classes = 10 → outputs.shape = [4, 10]（以batch_size=4为例）
	outputs = tensor([[ 2.10,  0.30, -1.00,  0.05,  0.1, -0.2, 1.0, 0.2, -0.1, 0.0],
                  [ 0.9,   0.8,   0.7,  -0.5,  0.1,  0.0, 0.2, 0.3, 0.1, -0.2],
                  [-1.2,  -0.3,   0.2,   0.5,  1.4,  0.0, 0.0, 0.1, 0.2, 0.3],
                  [ 0.0,   0.0,   0.0,   0.0,  0.0,  0.0, 0.0, 0.0, 0.0, 0.0]])
    含义：每行第 j 位是样本被预测为类别 j 的相对打分（越大表示越倾向该类）
    经过softmax后：
    	posteriors = F.softmax(outputs, dim=1)
    	shape：仍为 [batch_size, num_classes]
        例子（对上面第一行 softmax 后）：
        posteriors[0] ≈ tensor([0.831, 0.088, 0.012, 0.020, 0.024, 0.018, 0.054, 0.026, 0.021, 0.012])

# AttackNet
# 输入
attack_X：（有两种形式，可以是top_k，也可以是所有的，下面以完整的为例）
	shape（单样本）：[num_classes]（CIFAR10 为 10）
    batch 输入 shape：[batch_size, num_classes]，例如 [64,10]
    attack_in = tensor([[0.83,0.09,0.01,0.02,0.01,0.01,0.02,0.01,0.0,0.0],
                    [0.37,0.34,0.29,0,0,0,0,0,0,0],
                    [0.90,0.05,0.05,0,0,0,0,0,0,0],
                    [0.45,0.25,0.15,0.05,0.05,0.05,0,0,0,0]])  # shape [4,10]
    
# 输出
	logits shape：[batch_size, out_classes]，代码设 out_classes=2 → [batch_size, 2]
    attack_out = tensor([[0.2, 1.3],   # sample0 -> model认为更可能是 member (index1)
                     	[1.0, 0.1],    # sample1 -> model认为更可能是 non-member (index0)
                     	[0.01, 0.02]]) # sample2 uncertain
    含义：第 0 列对应 “非成员” 的分数，第 1 列对应 “成员” 的分数
    # 预测：
    _, pred = torch.max(attack_out, dim=1)  # pred = [1,0,1]
    # 真正的 membership labels（来自 prepare_attack_data）例如:
    labels = tensor([1, 0, 1], dtype=torch.long)  # shape [3]
~~~

